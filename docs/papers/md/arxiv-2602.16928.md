# Discovering Multiagent Learning Algorithms with Large Language Models

Discovering Multiagent Learning Algorithms with Large Language Models\correspondingauthor
lizun@google.com

# Discovering Multiagent Learning Algorithms with Large Language Models
Zun Li Google DeepMind John Schultz Google DeepMind Daniel Hennes Google DeepMind Marc Lanctot Google DeepMind 
###### Abstract

Much of the advancement of Multi-Agent Reinforcement Learning (MARL) in imperfect-information games has historically depended on manual iterative refinement of baselines. While foundational families like Counterfactual Regret Minimization (CFR) and Policy Space Response Oracles (PSRO) rest on solid theoretical ground, the design of their most effective variants often relies on human intuition to navigate a vast algorithmic design space. In this work, we propose the use of AlphaEvolve, an evolutionary coding agent powered by large language models, to automatically discover new multiagent learning algorithms. We demonstrate the generality of this framework by evolving novel variants for two distinct paradigms of game-theoretic learning. First, in the domain of iterative regret minimization, we evolve the logic governing regret accumulation and policy derivation, discovering a new algorithm, Volatility-Adaptive Discounted (VAD-)CFR. VAD-CFR employs novel, non-intuitive mechanismsâ€”including volatility-sensitive discounting, consistency-enforced optimism, and a hard warm-start policy accumulation scheduleâ€”to outperform state-of-the-art baselines like Discounted Predictive CFR+. Second, in the regime of population based training algorithms, we evolve training-time and evaluation-time meta strategy solvers for PSRO, discovering a new variant, Smoothed Hybrid Optimistic Regret (SHOR-)PSRO. SHOR-PSRO introduces a hybrid meta-solver that linearly blends Optimistic Regret Matching with a smoothed, temperature-controlled distribution over best pure strategies. By dynamically annealing this blending factor and diversity bonuses during training, the algorithm automates the transition from population diversity to rigorous equilibrium finding, yielding superior empirical convergence compared to standard static meta-solvers.

###### keywords: 
Multi-Agent Reinforcement Learning, Game Theory, Large Language Models, Meta-Learning 
## 1 Introduction

The field of Multi-Agent Reinforcement Learning has achieved remarkable milestones in recent years, reaching superhuman performance in domains ranging from Poker to real-time strategy games. These advances have been driven by a diverse array of methods, including game-theoretic regret minimization [Brown2019Pluribus] and population-based league training [Vinyals2019AlphaStar]. The practical performance of these algorithms relies heavily on specific structural choicesâ€”such as how regret is discounted over time or how a specific equilibrium solution concept is derived. Historically, the refinement of these choices has been a largely manual endeavor. Researchers must rely on intuition and trial-and-error to navigate a vast combinatorial space of potential update rules, often defaulting to mathematically tractable heuristics (e.g., linear averaging or fixed discounting) that may not be optimal.

In this work, we propose to overcome this limitation by automating the design process itself with Large Language Models (LLMs). We apply AlphaEvolve [novikov2025alphaevolve], a distributed evolutionary system powered by LLMs, to the domain of multi-agent learning. Unlike traditional hyperparameter optimization or genetic programming, AlphaEvolve leverages the code-generation capabilities of LLMs to perform semantic evolution. By treating the algorithmâ€™s source code as the genome, the system uses LLMs to act as intelligent genetic operatorsâ€”performing mutation to rewrite logic, introduce new control flows, and inject novel symbolic operations. This allows the search to transcend simple parameter tuning and discover entirely new mechanisms for equilibrium finding.

We validate the generality of this framework by applying it to the two dominant paradigms of imperfect-information game solving:

1. Evolving Counterfactual Regret Minimization (CFR) [Zinkevich2007CFR]: The CFR family of algorithms relies on recursive definitions for accumulating regret and deriving an average policy. State-of-the-art variants like DCFR [brown2019solving] and PCFR+ [farina2021faster] were developed by manually identifying specific weighting schemes or regret targets to accelerate convergence. We apply AlphaEvolve to the symbolic operations governing regret accumulation, policy aggregation, and current policy derivation. The search yields Volatility-Adaptive Discounted (VAD-)CFR, a novel variant that dynamically adjusts its discounting parameters based on the volatility of regret updates and utilizes a regret-magnitude weighted warm-start to construct the average strategy. This approach outperforms existing baselines across a diverse set of game benchmarks.

2. Evolving Policy-Space Response Oracles [Lanctot2017PSRO] (PSRO): PSRO generalizes the Double Oracle algorithm [mcmahan2003planning], expanding a population of policies by iteratively computing best responses and solving for a meta-strategy. Standard meta-solvers (e.g., Projected Replicator Dynamics or Uniform) enforce static trade-offs between exploration (expanding the game graph) and exploitation (refining the equilibrium). These static heuristics often fail to adapt to the changing topology of the empirical game during training. We apply AlphaEvolve to discover dynamic training-time and evaluation-time meta-solvers that optimize this schedule. The resulting variant, Smoothed Hybrid Optimistic Regret (SHOR-)PSRO, utilizes a hybrid update rule that mixes regret-based stability with aggressive greedy exploitation. By annealing the weight of these components over the training horizon, SHOR-PSRO empirically shows both algorithmic robustness and iteration efficiency.

The contributions of this paper are summarized as follows:

- 1.
We showcase using LLM-driven evolution to design multi-agent learning algorithms, moving beyond parameter tuning to symbolic code evolution.

- 2.
We identify VAD-CFR, an evolved algorithm that demonstrates how automated search can uncover update rules that are effective yet non-intuitive to human designers. In Appendix 7.2, we also present AOD-CFR, a variant discovered in an early trial that employs more conventional mechanisms while maintaining competitive performance.

- 3.
We demonstrate the versatility of the framework by evolving PSRO meta-solvers that improve convergence speed and stability automating the transition from exploration to exploitation, yielding a new variant, SHOR-PSRO.

By automating the discovery of these mechanisms, we anticipate that future game-theoretic solvers will derive from a blend of human ingenuity and AI-driven insights.

## 2 Game Theoretic Preliminaries

We formulate our problem within the framework of Extensive-Form Games (EFGs) with imperfect information, which models sequential interactions involving multiple agents and hidden information.

### 2.1 Extensive-Form Games and Exploitability

An NN-player extensive-form game is Î“=âŸ¨ð’©,â„‹,ð’µ,ð’œ,u,â„,ÏƒâŸ©\Gamma=\langle\mathcal{N},\mathcal{H},\mathcal{Z},\mathcal{A},u,\mathcal{I},\sigma\rangle, where ð’©={1,â€¦,N}\mathcal{N}=\{1,\dotsc,N\} denotes the set of players [ShohamLeytonBrown2008]. â„‹\mathcal{H} is the set of all possible histories (sequences of actions), where ð’µâŠ†â„‹\mathcal{Z}\subseteq\mathcal{H} represents terminal histories. For iâˆˆð’©i\in\mathcal{N}, let â„‹iâŠ†H\mathcal{H}_{i}\subseteq H represent the subset of histories where player ii acts. For any non-terminal history hh, ð’œâ€‹(h)\mathcal{A}(h) is the set of legal actions. The utility function ui:ð’µâ†’â„u_{i}:\mathcal{Z}\rightarrow\mathbb{R} assigns a payoff to player ii at a terminal node.

Crucially, imperfect information is modeled via Information SetsIâˆˆâ„iI\in\mathcal{I}_{i}. Specifically, â„i\mathcal{I}_{i} partitions the histories â„‹i\mathcal{H}_{i} such that player ii cannot distinguish between histories h,hâ€²âˆˆIh,h^{\prime}\in I (e.g., due to hidden cards). It is required that ð’œâ€‹(h)=ð’œâ€‹(hâ€²)\mathcal{A}(h)=\mathcal{A}(h^{\prime}) for all h,hâ€²âˆˆIh,h^{\prime}\in I, so for simplicity we denote the legal actions at II, ð’œâ€‹(I)\mathcal{A}(I). A strategy (or policy) Ïƒiâ€‹(I)\sigma_{i}(I) assigns a probability distribution over actions aâˆˆð’œâ€‹(I)a\in\mathcal{A}(I) for each information set. A strategy profile Ïƒ=(Ïƒ1,â€¦,ÏƒN)\sigma=(\sigma_{1},\dotsc,\sigma_{N}) determines the expected utility uiâ€‹(Ïƒ)u_{i}(\sigma). A Nash Equilibrium (NE) is a strategy profile Ïƒâˆ—\sigma^{*} such that neither player can increase their utility by deviating unilaterally:
uiâ€‹(Ïƒiâˆ—,Ïƒâˆ’iâˆ—)â‰¥uiâ€‹(Ïƒiâ€²,Ïƒâˆ’iâˆ—)âˆ€Ïƒiâ€²,âˆ€iâˆˆð’©u_{i}(\sigma_{i}^{*},\sigma_{-i}^{*})\geq u_{i}(\sigma_{i}^{\prime},\sigma_{-i}^{*})\quad\forall\sigma_{i}^{\prime},\forall i\in\mathcal{N}(1)
To measure the performance of our evolved algorithms, we use Exploitability. The exploitability of a strategy profile Ïƒ\sigma is the average of the incentives for players to deviate to their Best Response (BR):
Exploitabilityâ€‹(Ïƒ)=1Nâ€‹âˆ‘iâˆˆð’©(maxÏƒiâ€²â¡uiâ€‹(Ïƒiâ€²,Ïƒâˆ’i)âˆ’uiâ€‹(Ïƒ))\text{Exploitability}(\sigma)=\frac{1}{N}\sum_{i\in\mathcal{N}}\left(\max_{\sigma_{i}^{\prime}}u_{i}(\sigma_{i}^{\prime},\sigma_{-i})-u_{i}(\sigma)\right)(2)
In small or medium-sized games, we can compute this value exactly by traversing the full game tree.

### 2.2 Counterfactual Regret Minimization (CFR)

CFR is an iterative algorithm that minimizes counterfactual regret[Zinkevich2007CFR]. It decomposes the global regret minimization problem into independent local regret minimization problems at each information set.

Let Ï€Ïƒâ€‹(h)\pi^{\sigma}(h) be the probability of reaching history hh under strategy Ïƒ\sigma. We define Ï€âˆ’iÏƒâ€‹(h)\pi^{\sigma}_{-i}(h) as the contribution of all players exceptii (including chance) to reaching hh. The counterfactual value of reaching information set II and playing action aa is the expected utility given that player ii played to reach II:
viâ€‹(Ïƒ,I,a)=âˆ‘hâˆˆIÏ€âˆ’iÏƒâ€‹(h)â€‹âˆ‘zâˆˆð’µ,hâŠzÏ€Ïƒâ€‹(z|h,a)â€‹uiâ€‹(z)v_{i}(\sigma,I,a)=\sum_{h\in I}\pi_{-i}^{\sigma}(h)\sum_{z\in\mathcal{Z},h\sqsubset z}\pi^{\sigma}(z\penalty 10000\ |\penalty 10000\ h,a)u_{i}(z)(3)
The instantaneous counterfactual regret for not playing action aa at iteration tt is the difference between the counterfactual value of action aa and the expected value at II:
rtâ€‹(I,a)=viâ€‹(Ïƒt,I,a)âˆ’âˆ‘aâ€²âˆˆð’œâ€‹(I)Ïƒtâ€‹(I,aâ€²)â€‹viâ€‹(Ïƒt,I,aâ€²)r^{t}(I,a)=v_{i}(\sigma^{t},I,a)-\sum_{a^{\prime}\in\mathcal{A}(I)}\sigma^{t}(I,a^{\prime})v_{i}(\sigma^{t},I,a^{\prime})(4)
Standard CFR accumulates these values linearly over iterations TT:
RTâ€‹(I,a)=âˆ‘t=1Trtâ€‹(I,a)R^{T}(I,a)=\sum_{t=1}^{T}r^{t}(I,a)(5)
The current policy Ïƒt+1\sigma^{t+1} is derived from accumulated regret, typically using Regret Matching (RM), which assigns probabilities proportional to positive regret:
Ïƒt+1â€‹(I,a)=maxâ¡(Rtâ€‹(I,a),0)âˆ‘aâ€²maxâ¡(Rtâ€‹(I,aâ€²),0)\sigma^{t+1}(I,a)=\frac{\max(R^{t}(I,a),0)}{\sum_{a^{\prime}}\max(R^{t}(I,a^{\prime}),0)}(6)
The strategy Ïƒt\sigma^{t} at any single iteration may not converge to NE. Instead, CFR outputs the average strategyÏƒÂ¯T\bar{\sigma}^{T}, computed by weighting the iteration strategy Ïƒt\sigma^{t} by the playerâ€™s contribution to the reach probability Ï€iÏƒtâ€‹(I)\pi_{i}^{\sigma^{t}}(I).

CFR Variants: Several variants modify these update rules. For example, CFR+[tammelin2014solving] replaces the regret accumulation RTR^{T} with floor bounding maxâ¡(RTâˆ’1+rt,0)\max(R^{T-1}+r^{t},0) and uses linear averaging weights (wt=tw_{t}=t) for the average policy. Our work uses AlphaEvolve to search for optimal variations of these accumulation and derivation functions.

### 2.3 Policy Space Response Oracles (PSRO)

PSRO [Lanctot2017PSRO, bighashdel2024policy] acts as a meta-solver that generalizes the Double Oracle algorithm [mcmahan2003planning]. It operates on a higher level of abstraction called the Meta-Game (or Empirical Game [wellman2025empirical]). PSRO maintains a population of policies Î i={Ïƒi1,â€¦,Ïƒik}\Pi_{i}=\{\sigma_{i}^{1},\dots,\sigma_{i}^{k}\} for each player. The meta-game is represented by a payoff tensor MM, where entries Mij1â€‹â€¦â€‹jN=uiâ€‹(Ïƒ1j1,â€¦,ÏƒNjN)M_{i}^{j_{1}\dotsc j_{N}}=u_{i}(\sigma^{j_{1}}_{1},\dotsc,\sigma^{j_{N}}_{N}) correspond to the expected utility of player ii when policies from the population are pitted against each other.

The PSRO Loop: At each epoch kk, the algorithm performs three steps:

- 1.
Meta-Strategy Solver (MSS) at training-time: A solver computes a meta-strategy Ï•i\phi_{i} (a probability distribution over the population Î i\Pi_{i}). Common solvers include UniformÏ•iji=1/|Î i|\phi_{i}^{j_{i}}=1/|\Pi_{i}| and NashÏ•\phi which is a Nash Equilibrium of the current meta-game MM.

- 2.
Oracle (Best Response): A new policy Ïƒik+1\sigma_{i}^{k+1} is trained via Reinforcement Learning (or exact solving) to be a Best Response to the opponentâ€™s meta-strategy:
Ïƒik+1âˆˆargâ¡maxÏƒiâ¡ð”¼Ïƒâˆ’iâˆ¼Ï•âˆ’iâ€‹[uiâ€‹(Ïƒi,Ïƒâˆ’i)]\sigma_{i}^{k+1}\in{\arg\max}_{\sigma_{i}}\mathbb{E}_{\sigma_{-i}\sim\phi_{-i}}\left[u_{i}(\sigma_{i},\sigma_{-i})\right](7)
In this work, we utilize an exact oracle that computes the optimal best response to the meta-strategy, isolating the performance of the meta-solver from the variance of reinforcement learning.

- 3.
Expansion: The new policy is added to the population Î iâ†Î iâˆª{Ïƒik+1}\Pi_{i}\leftarrow\Pi_{i}\cup\{\sigma_{i}^{k+1}\}, and the payoff tensor MM is updated. In this work, we calculate the exact expected payoff value for each entry Mij1â€‹â€¦â€‹jNM_{i}^{j_{1}\dotsc j_{N}}, thereby eliminating the stochastic noise associated with Monte Carlo sampling.

To quantify performance, we employ a distinct evaluation-time meta-strategy solver to compute a distribution over the current policy population; this distribution serves as the basis for the exploitability metric. It is critical [wang2022evaluating] to distinguish this from the training process: the training-time MSS drives the generation of new policies (often prioritizing exploration), while the evaluation-time MSS ensures a robust evaluation of the populationâ€™s quality. We regard both the training-time and evaluation-time MSS as integral components of PSRO when employed as a single game-solver. Standard PSRO relies on fixed static meta-solvers for both training-time and eval-time (e.g., always using Nash or always using Uniform). However, the optimal meta-solver may change during trainingâ€”for instance, encouraging exploration (Uniform) early on and robustness (Nash) later. We aim to discover a dynamic meta-solver schedule.

## 3 Method: Automating Algorithm Discovery via AlphaEvolve

We propose a framework to automate the design of multi-agent learning algorithms by shifting the design process from manual heuristics to automated discovery. We utilize AlphaEvolve[novikov2025alphaevolve], an evolutionary coding agent that leverages Large Language Models (LLMs) to evolve executable code. We apply this framework to two distinct paradigms: Regret Minimization (CFR) and Population based Training (PSRO).

### 3.1 The AlphaEvolve Framework

AlphaEvolve combines the creative code-generation capabilities of LLMs with the rigorous selection pressure of evolutionary algorithms. Unlike traditional genetic programming which relies on random syntactic mutations, AlphaEvolve uses an LLM to propose semantically meaningful modifications to a code file. The process operates as follows:

- â€¢
Population Initialization: We initialize a population ð’«\mathcal{P} containing the standard implementations of the baseline algorithms (e.g., standard CFR code or Uniform PSRO code).

- â€¢
LLM-Driven Mutation: In each generation, a parent algorithm Aâˆˆð’«A\in\mathcal{P} is selected based on fitness. Notice that AlphaEvolve supports multi-objective optimization: if multiple fitness metrics are defined, one of them (say ff) will be randomly selected, and AA is then sampled in favor of high value of ff. The source code of AA is fed into an LLM (e.g., Gemini [comanici2025gemini]) with a prompt instructing it to "Modify the following code to improve fitness (reduce exploitability)." The LLM generates a new candidate variant Aâ€²A^{\prime} by applying several lines of code changes to AA.

- â€¢
Automated Evaluation: The candidate Aâ€²A^{\prime} is executed on a set of proxy games (e.g., Kuhn Poker). An automated evaluator computes its fitness scores (final negative exploitability).

- â€¢
Evolutionary Selection: Valid candidates are added to the population ð’«\mathcal{P}. This loop repeats, allowing the system to discover complex, non-intuitive optimizations.

### 3.2 Evolving Regret Minimization Code

To discover new variants of CFR, we expose the core update functions of the Counterfactual Regret Minimization algorithm to the AlphaEvolve agent. The search space is defined by the Python functions responsible for accumulating regret and updating the average policy. Specifically, we design the components with the primitives in Listing 1. This search space is expressive enough to encompass the entire family of known CFR variants as special cases. For instance, standard CFR uses eq (5) for RegretAccumulator and eq (6) for PolicyFromRegretAccumulator. These components are Python classes which allow for maintaining state across iterations, in contrast to pure functions.
Listing 1: The Python CFR code skeleton used as the search space for AlphaEvolve. The highlighted methods update_accumulate_regret, get_updated_current_policy, and update_accumulate_policy represent the evolvable components of the CFR algorithm.â¬‡classRegretAccumulator:"""Aclassthatupdatescumulativeregretataninformationset."""defupdate_accumulate_regret(self,info_state_node,iteration_number,cfr_regrets):"""Args:info_state_node:Datastructurewithcumulative_regretandcumulative_policy.iteration_number:CurrentCFRiteration.cfr_regrets:Counterfactualregrets(notyetaddedtonode).Returns:Updatedcumulativeregretdictionaryforeachaction."""...classPolicyFromRegretAccumulator:"""Aclassthatderivesthecurrentpolicyfromregret."""defget_updated_current_policy(self,info_state_node,iteration_number,cfr_regrets,previous_policy):"""Args:info_state_node:Datastructurewithcumulative_regret.iteration_number:CurrentCFRiteration.cfr_regrets:Counterfactualregrets(alreadyaddedtonode).previous_policy:Previouspolicyatthisinfoset.Returns:Updatedcurrentpolicydictionary."""...classPolicyAccumulator:"""Aclassthatupdatestheaveragepolicyduringtreetraversal."""defupdate_accumulate_policy(self,info_state_node,iteration_number,info_state_policy,cfr_regrets,reach_prob,counterfactual_reach_prob):"""Args:info_state_node:Datastructurewithcumulative_policy.iteration_number:CurrentCFRiteration.info_state_policy:Currentpolicyatthisinfoset.cfr_regrets:Counterfactualregrets(alreadyaddedtonode).reach_prob:Probabilityofreachingcurrenthistory(playerâ€™scontribution).counterfactual_reach_prob:Probabilityofreachingcurrenthistory(opponentsâ€™contribution).Returns:Updatedcumulativepolicydictionary."""...
### 3.3 Evolving Meta-Strategy Solvers Code

For PSRO, we evolve TrainMetaStrategySolver for generating the mixed strategies used during the oracle training phase, and EvalMetaStrategySolver for computing a strategy profile to report metrics such as exploitability. The code skeleton we designed is shown in Listing 2. This interface supports the representation of all standard baselines as special cases: for example, standard double oracle algorithm solves for a Nash equilibrium in the get_meta_strategy method for both solver classes.
Listing 2: The Python PSRO code skeleton used as the search space for AlphaEvolve. The highlighted methods TrainMetaStrategySolver and EvalMetaStrategySolver represent the evolvable components of the PSRO algorithm.â¬‡classTrainMetaStrategySolver:"""ReturnsmetastrategiestotrainagainstinPSRO."""defget_meta_strategy(self,game,policy_sets,meta_games):"""Returnsmetastrategiestotrainagainstinpolicy-spaceresponseoracles.Args:game:Thepyspielgameobject.policy_sets:Alistoflistsofpolicies,onelistperplayer.policy_sets[p][i]isplayerpâ€™si-thpolicy.len(policy_sets[p])==meta_games[0].shape[p].meta_games:Alistofn-dimensionalnumpyarrays,oneperplayer.Eacharrayhasshape(num_strats_p0,num_strats_p1,...,num_strats_pn-1)andmeta_games[p][i0,i1,...,in-1]isthepayoffofplayerpwhenplayerkchoosesstrategyik.Returns:Alistofmixed-strategies,oneforeachplayer.Eachmixedstrategyisalistofnon-negativeweights(notnecessarilynormalized).Itisusedtotrainbestresponseagainst."""...classEvalMetaStrategySolver:"""ReturnsmetastrategiesforevaluationinPSRO."""defget_meta_strategy(self,game,policy_sets,meta_games):"""Returnsmetastrategiesforevaluationinpolicy-spaceresponseoracles.Args:game:Thepyspielgameobject.policy_sets:Alistoflistsofpolicies,onelistperplayer.policy_sets[p][i]isplayerpâ€™si-thpolicy.len(policy_sets[p])==meta_games[0].shape[p].meta_games:Alistofn-dimensionalnumpyarrays,oneperplayer.Eacharrayhasshape(num_strats_p0,num_strats_p1,...,num_strats_pn-1)andmeta_games[p][i0,i1,...,in-1]isthepayoffofplayerpwhenplayerkchoosesstrategyik.Returns:Alistofmixed-strategies,oneforeachplayer.Eachmixedstrategyisalistofnon-negativeweights(notnecessarilynormalized).ItisusedforevaluationofthecurrentPSROpolicies.E.g.,computingexploitability."""...
### 3.4 Optimization Objective

We manually select a set ð’¢\mathcal{G} of training games for AlphaEvolve to compute |ð’¢|+1|\mathcal{G}|+1 fitness scores. These are the negative exploitability âˆ’Exploitabilityâ€‹(Aâ€‹(g)K)-\text{Exploitability}(A(g)_{K}) of the final strategy profile after KK iterations for each gâˆˆð’¢g\in\mathcal{G}, as well as their average
âˆ’1|ð’¢|â€‹âˆ‘gâˆˆð’¢Exploitabilityâ€‹(Aâ€‹(g)K).-\frac{1}{|\mathcal{G}|}\sum_{g\in\mathcal{G}}\text{Exploitability}(A(g)_{K}).
Here Aâ€‹(g)KA(g)_{K} denotes the strategy produced by algorithm AA on game gg at iteration KK. The reported algorithms are selected based on their average scores.

## 4 Experimental Evaluation
Figure 1: CFR variants performances.
### 4.1 Experimental Setup

To test the robustness and generalizability of the algorithms we discover, we adopted a rigorous evaluation protocol involving two distinct sets of games. The algorithmâ€™s architecture and hyperparameters were developed and tuned on a Training Set of four games. For both CFR and PSRO discoveries, we choose this set to be 3-player Kuhn Poker, 2-player Leduc Poker, 4-card Goofspiel, and 5-sided Liars Dice. Subsequently, the fixed algorithm was evaluated on a Test Set. In the main body of this paper, we present results on four larger and more difficult games as a representative subset of test games: 4-player Kuhn Poker, 3-player Leduc Poker, 5-card Goofspiel, and 6-sided Liarâ€™s Dice. The results of a full-sweep of 11 games are provided in Appendix 7.3. We utilized the OpenSpiel [lanctot2019openspiel] framework for all experiments. Our implementation of AlphaEvolve is backboned by Gemini 2.5 pro [comanici2025gemini].

### 4.2 Experimental Evaluation: VAD-CFR111Appendix 7.2 details an alternative variant (AOD-CFR) discovered on a different set of training games that also achieves robust results. Notably, this variant relies on more conventional mechanisms than VAD-CFR.

We evaluate the efficacy of the evolved algorithm, VAD-CFR, against established game-theoretic baselines. We benchmarked VAD-CFR against a suite of state-of-the-art regret minimization algorithms: standard CFR [Zinkevich2007CFR], CFR+ [tammelin2014solving], Linear CFR (LCFR), Discounted CFR (DCFR) [brown2019solving], Predictive CFR+ (PCFR+) [farina2021faster], Discounted Predictive CFR+ (DPCFR+) [xu2024minimizing], and a Hyperparameter Schedule-powered PCFR variant HS-PCFR+(30) [zhang2024faster]. Performance was quantified using exploitability (measured on a logarithmic scale) over a fixed horizon of 1000 iterations, e.g., K=1000K=1000. For all domains the exploitability scores are computed exactly. We use CFR+ as the seed program. The prompt we use is shown in Listing 8 in the Appendix.

#### 4.2.1 Discovered Algorithmic Architecture

The source code of the discovered VAD-CFR algorithm is provided in Listing 5 in Appendix 7.1. A high-level abstraction is shown in Listing 3. The evolutionary search discarded the standard linear averaging and static discounting of the CFR family in favor of three distinct, non-intuitive mechanisms:

- â€¢
Volatility-Adaptive Discounting (vs. Static Discounting): Standard DCFR applies fixed discount factors (Î±,Î²\alpha,\beta) to cumulative regrets. In contrast, VAD-CFR makes these parameters reactive. It tracks the volatility of the learning process via an Exponential Weighted Moving Average (EWMA) of the instantaneous regret magnitude. When volatility is high (indicating the strategy is in flux), the algorithm dynamically increases discounting to "forget" the unstable history faster; when volatility drops, it retains more history for fine-tuning.

- â€¢
Asymmetric Instantaneous Boosting: While DCFR applies asymmetry to accumulated history, VAD-CFR applies a novel asymmetry to the instantaneous update itself. Positive instantaneous regrets (actions that are currently good) are boosted by a factor of 1.11.1. This asymmetry enables the immediate exploitation of beneficial deviations, eliminating the lag associated with accumulation.

- â€¢
Hard Warm-Start & Regret-Magnitude Weighting: Perhaps the most interesting discovery is the policy accumulation schedule. Standard CFR variants begin averaging policies immediately at t=1t=1. In contrast, VAD-CFR enforces a hard warm-start333The LLM generated this threshold without knowing the fixed 1000-iteration evaluation horizon in its prompt context (Listing 8)., postponing the start of policy averaging until iteration 500. The underlying regret accumulation process remains fully active during this phase, ensuring the agent continues to learn and update its strategy. Furthermore, once accumulation begins, policies are weighted not just linearly (tt), but by the magnitude of the instantaneous regret. This acts as a sophisticated filter, ensuring the final equilibrium approximation is constructed only from high-information iterations, preventing early-stage noise from polluting the solution quality. Warm starting mechanisms have been studied in a previous work [brown2016strategy].

Listing 3: High-level abstraction of VAD-CFR.â¬‡classRegretAccumulator:"""Volatility-AdaptiveDiscounting&AsymmetricBoosting."""defupdate_accumulate_regret(self,info_state_node,iteration_number,cfr_regrets):#1.Volatility&AdaptiveDiscountCalculationinst_mag=max((abs(r)forrincfr_regrets.values()),default=0.0)self.ewma=0.1*inst_mag+0.9*self.ewmavolatility=min(1.0,self.ewma/2.0)alpha=max(0.1,1.5-0.5*volatility)beta=-0.1-0.5*volatilitybeta=min(alpha,beta)t_plus_one=float(iteration_number)+1.0disc_pos=(t_plus_one**alpha)/(t_plus_one**alpha+1.0)disc_neg=(t_plus_one**beta)/(t_plus_one**beta+1.0)updated_regrets={}fora,rincfr_regrets.items():#2.AsymmetricInstantaneousBoostingr_boosted=r*1.1ifr>0elser#3.Sign-DependentHistoryDiscountingprev_R=info_state_node.cumulative_regret.get(a,0.0)discount=disc_posifprev_R>=0elsedisc_neg#4.UpdatewithNegativeCapnew_R=(prev_R*discount)+r_boostedupdated_regrets[a]=max(-20.0,new_R)returnupdated_regretsclassPolicyFromRegretAccumulator:"""Derivespolicyfromaâ€™FutureProjectionâ€™ofregrets."""defget_updated_current_policy(self,info_state_node,iteration_number,cfr_regrets,previous_policy):#1.Consistency:Re-calculateexactadaptiveparamsinst_mag=max((abs(r)forrincfr_regrets.values()),default=0.0)volatility=min(1.0,inst_mag/2.0)#2.DecayingOptimismSchedulebase_optimism=1.0/(1.0+iteration_number/100.0)optimism=base_optimism*max(0.0,1.0-0.5*volatility)policy={}sum_pos_regret=0.0foraininfo_state_node.legal_actions:#3.ProjectedRegretCalculationr_boosted=cfr_regrets.get(a,0.0)ifr_boosted>0:r_boosted*=1.1prev_R=info_state_node.cumulative_regret.get(a,0.0)discount=get_adaptive_discount_pos(prev_R)ifprev_R>=0elseget_adaptive_discount_neg(prev_R)proj_R=(prev_R*discount)+r_boosted+optimism#4.Non-LinearProbabilityScalingifproj_R>0:scaled_r=proj_R**1.5policy[a]=scaled_rsum_pos_regret+=scaled_relse:policy[a]=0.0returnnormalize(policy)ifsum_pos_regret>0elseuniform(policy)classPolicyAccumulator:"""Warm-Start&MagnitudeWeighting."""defupdate_accumulate_policy(self,info_state_node,iteration_number,info_state_policy,cfr_regrets,reach_prob,counterfactual_reach_prob):#1.HardWarm-Startifiteration_number<500:returninfo_state_node.cumulative_policy#2.AdaptiveGamma(PolynomialAveraging)inst_mag=max((abs(r)forrincfr_regrets.values()),default=0.0)volatility=min(1.0,inst_mag/2.0)gamma=min(4.0,2.0+1.5*volatility)#3.Multi-FactorWeightingw_time=(float(iteration_number)+1.0)**gammaw_mag=(1.0+(inst_mag/2.0))**0.5w_stable=1.0/(1.0+inst_mag**1.5)final_weight=w_time*w_mag*w_stable#4.UpdateCumulativePolicyupdated_policy={}fora,probininfo_state_policy.items():prev_P=info_state_node.cumulative_policy.get(a,0.0)updated_policy[a]=prev_P+(final_weight*reach_prob*prob)returnupdated_policy
#### 4.2.2 Empirical Analysis: Training Domain

As shown in Figure 1, VAD-CFR demonstrates superior convergence across the training set. For 3-player Kuhn Poker, VAD-CFR achieves significantly lower exploitability than all baselines. For Leduc Poker and 4-card Goofspiel, the algorithm maintains a steeper convergence slope compared to DPCFR+ and other state-of-the-art variants. In 5-sided Liars Dice, VAD-CFR exhibits robust performance, effectively managing the larger state space through its adaptive discounting and boosting mechanisms.

#### 4.2.3 Generalization Capabilities: Test Domain

Results illustrated in Figure 1 highlights its generalization. In 3-player Leduc Poker, VAD-CFR reaches exploitability levels below 10âˆ’310^{-3} while most baselines plateau at higher levels. In 6-sided Liarâ€™s Dice, VAD-CFR continues to match established baselines like DCFR, suggesting that its evolved mechanisms for managing regret scaling and noise are highly effective across different game topologies.
Figure 2: PSRO variants performances.
Overall, VAD-CFR demonstrates efficient convergence rate and generalization across a broad variety of domains: as shown in Figure 3 in the Appendix, VAD-CFR matches or surpasses previous state-of-the-art performances in 10 of the 11 games, with 4-player Kuhn Poker as the only exception.

### 4.3 Experimental Evaluation: SHOR-PSRO

Next, we evaluated the performance of the evolved SHOR-PSRO algorithm, comparing its ability to reduce exploitability against standard meta-solver baselines. We use the exploitability at the K=K=100-th PSRO iteration as the metric. We employ an exact best response oracle via value iteration, where at each state it uniformly randomizes among actions with the same optimal values. We benchmarked SHOR-PSRO against standard established meta-solver baselines: Uniform, Nash equilibrium via linear program for 2-player games, AlphaRank [mullergeneralized], Projected Replicator Dynamics (PRD) [Lanctot2017PSRO], and Regret Matching (RM). We use Uniform as the initial program for both solver classes. The prompt we use is shown in Listing 9 in the Appendix.

#### 4.3.1 Discovered Algorithmic Architecture

The source code of the discovered SHOR-PSRO algorithm is provided in Listing 6 in Appendix 7.1. A high-level abstraction is in Listing 4. The evolutionary search yielded a â€œHybridâ€ meta-solver that constructs a meta-strategy Ïƒ\sigma by linearly blending two distinct components: Optimistic Regret Matching (ORM) and a Smoothed Best Pure Strategy.

- â€¢
Hybrid Blending Mechanism: At every internal solver iteration, the meta-strategy is computed as:
Ïƒhâ€‹yâ€‹bâ€‹râ€‹iâ€‹d=(1âˆ’Î»)â‹…ÏƒOâ€‹Râ€‹M+Î»â‹…ÏƒSâ€‹oâ€‹fâ€‹tâ€‹mâ€‹aâ€‹x\sigma_{hybrid}=(1-\lambda)\cdot\sigma_{ORM}+\lambda\cdot\sigma_{Softmax}(8)
where ÏƒOâ€‹Râ€‹M\sigma_{ORM} provides the stability of regret minimization, and ÏƒSâ€‹oâ€‹fâ€‹tâ€‹mâ€‹aâ€‹x\sigma_{Softmax} is a Boltzmann distribution over pure strategies that aggressively biases the solver toward high-reward modes. The blending factor Î»\lambda controls the trade-off between these dynamics.

- â€¢
Dynamic Annealing Schedule: Unlike standard PSRO which uses fixed solvers, SHOR-PSRO employs a dynamic schedule for the training-time solver. Over the course of the PSRO iterations:

- â€“
The blending factor Î»\lambda anneals from 0.3â†’0.050.3\rightarrow 0.05, gradually shifting focus from greedy exploitation to robust equilibrium finding.

- â€“
An explicit diversity bonus is applied to the payoff gains, which also decays from 0.05â†’0.0010.05\rightarrow 0.001, ensuring early expansion of the game graph followed by late-stage refinement.

- â€¢
Training vs. Evaluation Asymmetry: The search discovered distinct configurations for training and evaluation. The Training Solver utilizes the dynamic annealing schedule described above and returns the average strategy over internal iterations to ensure stability. In contrast, the Evaluation Solver employs a fixed, low blending factor (Î»=0.01\lambda=0.01) and returns the last-iterate strategy. This decoupling allows the algorithm to explore safely during training while providing reactive, low-noise exploitability estimates during evaluation.

Listing 4: High-level abstraction of SHOR-PSRO.â¬‡classTrainMetaStrategySolver:defget_meta_strategy(self,game,policy_sets,meta_games):"""HybridTrainingSolverwithFullParameterAnnealing"""#1.AnnealingSchedules(Exploration->Exploitation)prog=min(1.0,self.current_psro_iter/75.0)blend=0.30-(0.25*prog)#Blend:0.3->0.05div=0.05-(0.049*prog)#Diversity:0.05->0.001temp=0.50-(0.49*prog)#Temp:0.5->0.01#2.AdaptiveSolverIterationspop_size=len(meta_games[0])n_iters=int(1000+20*(pop_size-1))#3.HybridInternalLoop(TrainingreturnsAverage)return_hybrid_solver(meta_games,n_iters,blend,div,temp,momentum=0.5,return_avg=True)classEvalMetaStrategySolver:defget_meta_strategy(self,game,policy_sets,meta_games):"""EvaluationSolver(FixedParameters,Last-Iterate)"""#1.FixedParametersforPureExploitationblend,div,temp=0.01,0.0,0.001#2.AggressiveIterationScaleforConvergencepop_size=len(meta_games[0])n_iters=int(8000+50*(pop_size-1))#3.HybridLoopreturnsLast-Iterateforevalreactivityreturn_hybrid_solver(meta_games,n_iters,blend,div,temp,momentum=0.2,return_avg=False)def_hybrid_solver(meta_games,n_iters,blend,div,temp,momentum,return_avg):sigma=uniform_strategy(meta_games)sigma_avg=zeros_like(sigma)foriinrange(n_iters):#A.OptimisticRegretMatching(ORM)gains=compute_payoffs(meta_games,sigma)+div*(1-sigma)gains_norm=normalize_scale(apply_momentum(gains,beta=momentum))sigma_orm=regret_matching(gains_norm)#B.SmoothedBestPureStrategy(Softmax)exp_vals=compute_expected_values(meta_games,sigma)sigma_pure=softmax(exp_vals,temperature=temp)#C.HybridBlendingsigma=((1-blend)*sigma_orm)+(blend*sigma_pure)ifreturn_avg:sigma_avg+=sigmareturnnormalize(sigma_avg)ifreturn_avgelsesigma
#### 4.3.2 Empirical Analysis: Training Domain

As shown in Figure 2, SHOR-PSRO demonstrates superior convergence speed and stability across the training set. In simpler domains like Kuhn Poker, SHOR-PSRO achieves exploitability levels (<10âˆ’3<10^{-3}) significantly faster than PRD or RM. This acceleration is likely attributable to the "Hybrid Blending" mechanism, which allows the solver to leverage the stability of regret minimization while aggressively exploiting high-reward modes via the smoothed softmax component.

#### 4.3.3 Generalization Capabilities: Test Domain

To assess the robustness of the evolved mechanism, we evaluated SHOR-PSRO on a Test Set comprising larger and more complex game variants. The results, illustrated in Figure 2, highlight the algorithmâ€™s generalization capabilities. In 3-player Leduc Poker, the meta-game landscape becomes significantly more chaotic due to multi-agent dynamics. Despite this, SHOR-PSRO consistently matches or outperforms the best-performing baselines. In the most demanding test case, Liarâ€™s Dice (6 sides), SHOR-PSRO demonstrates a clear advantage. While static solvers struggle with the expanded branching factor, the hybrid solverâ€™s ability to bias towards "smoothed best pure strategies" allows it to navigate the expanded game graph efficiently. The results suggest that the evolved evaluation-time asymmetryâ€”using a fixed, low-noise configuration for measurementâ€”ensures that the exploitability metric accurately reflects the populationâ€™s growing strength without being obscured by exploration noise.

Overall, SHOR-PSRO proves to be a highly robust meta-solver, capable of generalizing to unseen game topologies without requiring manual re-tuning of its annealing schedules. As shown in Figure 4 in the Appendix, SHOR-PSRO matches or surpasses state-of-the-art performances in 8 of the 11 games.

## 5 Related Work

Our work sits at the intersection of game-theoretic multi-agent learning, meta-learning, and automated algorithmic discovery. Here, we review the manual heuristic design that characterizes the current state-of-the-art and contrast it with our automated approach.

Significant research effort has been dedicated to improving the convergence speed of CFR through specific weighting schemes and regret targets. Notable variants include CFR+ [tammelin2014solving], which introduced floor bounding and linear averaging; Discounted CFR (DCFR) [brown2019solving], which applies fixed discount factors to historical regrets; and Predictive CFR+ (PCFR+) [farina2021faster], which utilizes predictive Blackwell approachability. However, these improvements were largely derived through human intuition and trial-and-error to navigate the vast combinatorial space of potential update rules. In contrast to strategy-based warm starting [brown2016strategy], which relies on discrete, heuristic-driven resets, VAD-CFR employs a static warmup schedule with continuous volatility-based discounting to regulate regret retention.

For larger games where traversing the full tree is infeasible, Policy Space Response Oracles (PSRO) [bighashdel2024policy] generalizes the Double Oracle algorithm [mcmahan2003planning] by iteratively expanding a population of policies via exact or reinforcement learning best responses. While PSRO rests on solid theoretical ground [zhang2024exponential], its practical application faces challenges in terms of convergence speed and population quality. Recent surveys highlight the diversity of PSRO variants [bighashdel2024policy], but like CFR, the design of meta-solver schedules has traditionally been static or manually tuned rather than dynamically evolved.

The research of automating machine learning algorithm design has been evolving for both neural-based approaches and symbolic-based approaches. Meta-learning approaches such as meta-RL [xu2018meta, oh2025discovering] and meta-learning optimizers [metz2019understanding] have parameterized update rules using neural networks to optimize learning dynamics. For symbolic ML discovery, a foundational work in this domain is AutoML-Zero [real2020automl], which demonstrated that complete machine learning algorithms could be evolved from scratch using basic mathematical operations. A following work also applies program search for discovering optimizers [chen2023symbolic]. The authors of [coevolving] also studied discovering novel symbolic reinforcement learning algorithms.

Within the specific domain of multi-agent learning, there has been prior work on automating algorithm design [fengneural, xu2022autocfr, xu2024dynamic]. More directly related, one can meta-learn the regret minimization algorithm directly via a CFR-like no-regret framework from examples [Sychrovsky24]. However, these approaches often faced limitations: they either operated within a relatively constrained search space or relied on neural parameterizations that hindered interpretability. Our work builds directly on AlphaEvolve [novikov2025alphaevolve], utilizing Large Language Models (LLMs) to perform semantic mutation on interpretable code, bridging the gap between expressive neural meta-learning and symbolic discovery. This approach has already been shown great success in the field of math [georgiev2025mathematical] and combinatorial algorithms [nagda2025reinforced].

## 6 Conclusion

In this work, we proposed a methodology for using LLM-driven evolution to automate the design of multi-agent learning algorithms, effectively shifting the paradigm from manual heuristic tuning to symbolic code evolution. By leveraging AlphaEvolve, we demonstrated that algorithm design can be treated as a search problem within a combinatorial space of symbolic operations, allowing for the discovery of complex, non-intuitive optimization mechanisms.

First, we identified VAD-CFR, which breaks from the tradition of static discount schedules used in DCFR. By introducing volatility-adaptive parameters, instantaneous regret boosting, and a warm-start for policy averaging, VAD-CFR demonstrates that filtering out early-stage noise is as critical as optimizing late-stage convergence. These mechanisms significantly outperform state-of-the-art baselines in domains where exact exploitability is computable.

Second, we addressed the efficiency challenges of Policy Space Response Oracles (PSRO) by evolving SHOR-PSRO. This algorithm utilizes a hybrid meta-solver that blends Optimistic Regret Matching with a smoothed, temperature-controlled best response. By automating the annealing of this blending factor, SHOR-PSRO effectively manages the transition from population diversity to equilibrium refinement, yielding solvers that are significantly more effective for equilibrium finding in large-scale scenarios.

Our results suggest that the automated discovery of algorithmic asymmetriesâ€”specifically those that manage regret scaling and dynamic mixing schedulesâ€”can yield solvers that are elusive to human intuition but highly effective in practice. Future work will explore the application of this evolutionary framework to fully deep reinforcement learning agents and the discovery of cooperative mechanisms in general-sum games.

## Acknowledgements

We thank Ian Gemp and Xidong Feng for their valuable discussions and comments on this work.

## References

## 7 Appendix

### 7.1 Source code of discovered algorithms
Listing 5: VAD-CFRâ¬‡classRegretAccumulator:"""AclassthatupdatescumulativeregretusingAdaptiveDiscountingwithseparatediscountingforpositiveandnegativeregrets,andinstantaneousregretboosting."""@staticmethoddef_calculate_adaptive_params(iteration_number,cfr_regrets,base_alpha,base_beta,volatility_sensitivity,max_expected_instantaneous_regret,ewma_decay_factor,current_ewma_magnitude,):"""Calculatesadaptivediscountingparametersforagiveniteration.ThisstaticmethodcentralizesthelogicforEWMAvolatility,adaptivealpha/beta,anddiscountfactorstoensureconsistencyacrosscomponents."""t_plus_one=float(iteration_number+1)instantaneous_regret_magnitude=max((abs(r)forrincfr_regrets.values()),default=0.0)ifiteration_number==0:projected_ewma=instantaneous_regret_magnitudeelse:projected_ewma=(ewma_decay_factor*instantaneous_regret_magnitude+(1.0-ewma_decay_factor)*current_ewma_magnitude)ifmax_expected_instantaneous_regret>0:normalized_volatility=min(1.0,projected_ewma/max_expected_instantaneous_regret)else:normalized_volatility=0.0effective_alpha=max(0.1,base_alpha-volatility_sensitivity*normalized_volatility)effective_beta=base_beta-volatility_sensitivity*normalized_volatilityeffective_beta=min(effective_alpha,effective_beta)discount_factor_positive=(t_plus_one**effective_alpha)/(t_plus_one**effective_alpha+1.0)discount_factor_negative=(t_plus_one**effective_beta)/(t_plus_one**effective_beta+1.0)returnprojected_ewma,normalized_volatility,discount_factor_positive,discount_factor_negativedef__init__(self,base_alpha=1.5,base_beta=-0.1,volatility_sensitivity=0.5,max_expected_instantaneous_regret=2.0,instantaneous_regret_boost_factor=1.1,ewma_decay_factor=0.1,negative_regret_cap=-20.0):"""Initializestheregretaccumulatorwithadaptivediscountingparameters.Args:base_alpha:Thebaselineexponentfordiscountingpositivecumulativeregrets.base_beta:Thebaselineexponentfordiscountingnegativecumulativeregrets.volatility_sensitivity:Controlshowstronglytheinstantaneousregretmagnitudeinfluencestheadaptivealpha/beta.Ahighervaluemeanstheexponentswillbemorereducedbyhighvolatility.max_expected_instantaneous_regret:Anestimateofthemaximumpossibleinstantaneousregretmagnitude,usedfornormalizingthevolatility.instantaneous_regret_boost_factor:Boostfactorforpositiveinstantaneousregrets.Afactor>1.0makesthealgorithmmorereactivetocurrentgoodactions.ewma_decay_factor:DecayfactorfortheEWMAofinstantaneousregretmagnitude.negative_regret_cap:Theminimumvalueforcumulativeregret,topreventregretlock-inandimproveadaptability."""self._base_alpha=base_alphaself._base_beta=base_betaself._volatility_sensitivity=volatility_sensitivityself._max_expected_instantaneous_regret=max_expected_instantaneous_regretself._instantaneous_regret_boost_factor=instantaneous_regret_boost_factorself._ewma_decay_factor=ewma_decay_factorself._negative_regret_cap=negative_regret_capself._ewma_instantaneous_regret_magnitude=0.0defupdate_accumulate_regret(self,info_state_node,iteration_number,cfr_regrets):"""UpdatescumulativeregretforeachactionataninformationsetusingADCFRP.Cumulativeregretsarenowsigned.Args:info_state_node:adatastructurecorrespondingtoaninformationsetwithcumulative_regretandcumulative_policystored.iteration_number:thecurrentCFRiteration(0-indexed).cfr_regrets:theinstantaneouscounterfactualregretsofthecurrentpolicyatthecurrentinformationset.cfr_regretshavenâ€™tbeenaddedtoinfo_state_node.cumulative_regret.Returns:updatedcumulativeregretforeachactionatthecurrentinformationset(signed)."""#Centralizeadaptiveparametercalculationtoensureconsistencyandreduceredundancy.(self._ewma_instantaneous_regret_magnitude,_,#normalized_volatilityisnotusedherediscount_factor_positive,discount_factor_negative,)=RegretAccumulator._calculate_adaptive_params(iteration_number=iteration_number,cfr_regrets=cfr_regrets,base_alpha=self._base_alpha,base_beta=self._base_beta,volatility_sensitivity=self._volatility_sensitivity,max_expected_instantaneous_regret=self._max_expected_instantaneous_regret,ewma_decay_factor=self._ewma_decay_factor,current_ewma_magnitude=self._ewma_instantaneous_regret_magnitude,)updated_cumulative_regret={}foractionincfr_regrets:old_regret=info_state_node.cumulative_regret[action]instantaneous_regret_component=cfr_regrets[action]ifinstantaneous_regret_component>0:instantaneous_regret_component*=self._instantaneous_regret_boost_factor#Applydifferentdiscountfactorsbasedonthesignoftheoldregret.ifold_regret>=0:discounted_old_regret=discount_factor_positive*old_regretelse:discounted_old_regret=discount_factor_negative*old_regretnew_regret=discounted_old_regret+instantaneous_regret_component#Capthenegativeregrettopreventlock-inandimproveadaptability.new_regret=max(self._negative_regret_cap,new_regret)#Crucially,regretsareNOTclippedtozerohere.Theycanbenegative.updated_cumulative_regret[action]=new_regretreturnupdated_cumulative_regretclassPolicyFromRegretAccumulator:"""Aclassthatobtainsacurrentpolicyfromaconsistentoptimisticprojectionofregrets.Italignsthepolicygenerationwiththeadaptive,asymmetric,andboostedlogicfromRegretAccumulator."""def__init__(self,initial_optimism_factor=1.0,optimism_decay_factor=100.0,positive_policy_exponent=1.5,base_alpha=1.5,base_beta=-0.1,volatility_sensitivity=0.5,max_expected_instantaneous_regret=2.0,instantaneous_regret_boost_factor=1.1,ewma_decay_factor=0.1):"""InitializesthePolicyFromRegretAccumulatorwithparametersconsistentwithRegretAccumulator.Args:initial_optimism_factor:Theinitialweightingfortheinstantaneousregretcomponentintheprojection.optimism_decay_factor:Controlshowquicklytheoptimismweightdecays.positive_policy_exponent:Exponentfornon-linearscalingofpositiveregrets.base_alpha:Thebaselineexponentfordiscountingpositiveregrets.base_beta:Thebaselineexponentfordiscountingnegativeregrets.volatility_sensitivity:Controlsinfluenceofvolatilityonbothdiscountingandoptimismdampening.max_expected_instantaneous_regret:Usedfornormalizingvolatility.instantaneous_regret_boost_factor:Boostfactorforpositiveinstantaneousregrets.ewma_decay_factor:DecayfactorfortheEWMAofvolatility."""self._initial_optimism_factor=initial_optimism_factorself._optimism_decay_factor=optimism_decay_factorself._positive_policy_exponent=positive_policy_exponentself._base_alpha=base_alphaself._base_beta=base_betaself._volatility_sensitivity=volatility_sensitivityself._max_expected_instantaneous_regret=max_expected_instantaneous_regretself._instantaneous_regret_boost_factor=instantaneous_regret_boost_factorself._ewma_decay_factor=ewma_decay_factorself._ewma_instantaneous_regret_magnitude=0.0defget_updated_current_policy(self,info_state_node,iteration_number,cfr_regrets,previous_policy):"""ObtainsthecurrentpolicyusingaprojectionthatisconsistentwiththeRegretAccumulatorâ€™supdaterule.Thismethodcreatesatighterfeedbackloopbybasingthecurrentpolicyonaprojectionofwhattheregretswillbe*after*thecurrentiterationâ€™supdate.Thisprojectionusesthesameadaptive,asymmetricdiscountingandboostinglogicasthemainregretaccumulationstep."""#Centralizeadaptiveparametercalculation,ensuringconsistencywithRegretAccumulator.(self._ewma_instantaneous_regret_magnitude,normalized_volatility,discount_factor_positive,discount_factor_negative,)=RegretAccumulator._calculate_adaptive_params(iteration_number=iteration_number,cfr_regrets=cfr_regrets,base_alpha=self._base_alpha,base_beta=self._base_beta,volatility_sensitivity=self._volatility_sensitivity,max_expected_instantaneous_regret=self._max_expected_instantaneous_regret,ewma_decay_factor=self._ewma_decay_factor,current_ewma_magnitude=self._ewma_instantaneous_regret_magnitude,)base_optimism=self._initial_optimism_factor/(1.0+float(iteration_number)/self._optimism_decay_factor)#Dampenoptimismduringvolatileperiodstoincreasestability.optimism_dampening_factor=max(0.0,1.0-self._volatility_sensitivity*normalized_volatility)optimism_strength=base_optimism*optimism_dampening_factoraction_to_projected_regret={}foractionininfo_state_node.legal_actions:old_cumulative_regret=info_state_node.cumulative_regret.get(action,0.0)instantaneous_regret=cfr_regrets.get(action,0.0)instantaneous_regret_component=instantaneous_regretifinstantaneous_regret_component>0:instantaneous_regret_component*=self._instantaneous_regret_boost_factorifold_cumulative_regret>=0:discounted_old_regret=discount_factor_positive*old_cumulative_regretelse:discounted_old_regret=discount_factor_negative*old_cumulative_regretprojected_regret=discounted_old_regret+optimism_strength*instantaneous_regret_componentaction_to_projected_regret[action]=projected_regretpositive_scaled_projected_regrets={action:(max(0.0,regret)**self._positive_policy_exponent)foraction,regretinaction_to_projected_regret.items()}sum_positive_scaled_projected_regrets=sum(positive_scaled_projected_regrets.values())info_state_policy={}ifsum_positive_scaled_projected_regrets>0:foraction,scaled_regretinpositive_scaled_projected_regrets.items():info_state_policy[action]=scaled_regret/sum_positive_scaled_projected_regretselse:num_legal_actions=len(info_state_node.legal_actions)foractionininfo_state_node.legal_actions:info_state_policy[action]=1.0/num_legal_actionsreturninfo_state_policyclassPolicyAccumulator:"""Aclassthatupdatescumulativepolicyusingregret-informedweightedaveragingwithawarmupperiod."""def__init__(self,base_gamma=2.0,gamma_max=4.0,gamma_volatility_sensitivity=1.5,warmup_iterations=500,stability_exponent=1.5,max_expected_instantaneous_regret=2.0,regret_magnitude_weighting_exponent=0.5):#Newparameter"""InitializesthePolicyAccumulatorwithadaptivegammaparametersandregret-magnitudeweighting.Args:base_gamma:Thebaselineexponentforpolynomialweightingofpolicies.gamma_max:Themaximumvaluetheadaptivegammacanreach.gamma_volatility_sensitivity:Controlshowstronglyvolatilityinfluencesgamma.warmup_iterations:Numberofinitialiterationstoskipforpolicyaveraging.stability_exponent:Exponentforthestabilityfactorbasedonregretmagnitude.max_expected_instantaneous_regret:Normalizationfactorforinstantaneousregretmagnitude.regret_magnitude_weighting_exponent:Exponentforup-weightingpoliciesbasedontheabsolutemagnitudeofinstantaneousregrets.Highervaluesgivemoreemphasistopoliciesfromiterationswithlargeregrets."""self._base_gamma=base_gammaself._gamma_max=gamma_maxself._gamma_volatility_sensitivity=gamma_volatility_sensitivityself._warmup_iterations=warmup_iterationsself._stability_exponent=stability_exponentself._max_expected_instantaneous_regret=max_expected_instantaneous_regretself._regret_magnitude_weighting_exponent=regret_magnitude_weighting_exponent#Storeddefupdate_accumulate_policy(self,info_state_node,iteration_number,info_state_policy,cfr_regrets,reach_prob,counterfactual_reach_prob,):"""Updatescumulativepolicyusingdelayed,regret-informed,andregret-magnitudeweightedaveraging."""ifiteration_number<self._warmup_iterations:returninfo_state_node.cumulative_policy#Calculateinstantaneousregretmagnitude(L-infinitynorm)forthisiterationinstantaneous_regret_magnitude=max((abs(r)forrincfr_regrets.values()),default=0.0)#Normalizevolatilityusingthesharedparameterifself._max_expected_instantaneous_regret>0:normalized_volatility=min(1.0,instantaneous_regret_magnitude/self._max_expected_instantaneous_regret)else:normalized_volatility=0.0#Adaptgammabasedonvolatility:highervolatility->highergammaeffective_gamma=self._base_gamma+self._gamma_volatility_sensitivity*normalized_volatilityeffective_gamma=min(self._gamma_max,effective_gamma)#Standardpolynomialweightinggivesmoreweighttolateriterations,nowwithadaptivegamma.temporal_weight=(float(iteration_number)+1.0)**effective_gamma#CalculateastabilityfactorfromtheL-infinitynormofinstantaneousregrets.#Higherregretmagnitude->lowerstabilityfactor,usingL-infinitynormforconsistency.regret_stability_factor=1.0/(1.0+instantaneous_regret_magnitude**self._stability_exponent)#NEW:RegretMagnitudeWeightingFactor#Policiesfromiterationswithhigherregretmagnitudecontributemoretotheaverage.#Thisfactorbooststheweight,withhighervaluesoftheexponentgivingmoreemphasis.#Ensureitâ€™sneverzerotoavoiddivisionbyzeroorcompletelynullifyingweight.regret_magnitude_factor=(1.0+(instantaneous_regret_magnitude/self._max_expected_instantaneous_regret))**self._regret_magnitude_weighting_exponentregret_magnitude_factor=max(0.1,regret_magnitude_factor)#Ensureminimumvaluetoavoidzeroweightifnormalizationresultsinverysmallnumber#Thefinalweightcombinesthetemporal,stability,andregret-magnitude-basedcomponents.weight=temporal_weight*regret_stability_factor*regret_magnitude_factorreturn{action:(info_state_node.cumulative_policy[action]+weight*reach_prob*info_state_policy[action])foractionininfo_state_policy}Listing 6: SHOR-PSROâ¬‡importnumpyasnpdef_smoothed_best_pure_strategy(payoff_vec,temperature=1.0):"""Computesasmootheddistributionbiasedtowardsthebestpurestrategy.Thesoftmaxfunctionensuresthatstrategieswithhigherpayoffsaregivenhigherprobability,withâ€™temperatureâ€™controllingthesharpnessofthedistribution.Alowertemperaturemakesthedistributionmoreconcentratedonthebeststrategy,whileahighertemperatureleadstoamoreuniformdistribution."""#Subtractmaxpayofffornumericalstability(standardsoftmaxtrick)stable_payoffs=payoff_vec-np.max(payoff_vec)exp_payoffs=np.exp(stable_payoffs/temperature)sum_exp_payoffs=np.sum(exp_payoffs)ifsum_exp_payoffs>1e-12:#Avoiddivisionbyzeroreturnexp_payoffs/sum_exp_payoffselse:#Fallbacktouniformdistributionifallexponentiatedpayoffsare#effectivelyzero(e.g.,duetoverylowtemperatureandnegativepayoffs,#orallpayoffsbeingidenticalafterstabilization).returnnp.ones_like(payoff_vec)/len(payoff_vec)def_hybrid_orm_solver(meta_games,iterations,blending_factor=0.0,temperature=0.1,momentum_beta=0.0,gain_normalization=True,diversity_bonus_coeff=0.0,return_average_strategy=True):#New:Flagtoreturnaverageorlast-iteratestrategy"""Computesmeta-strategiesusingOptimisticRegretMatching+enhancedwithoptimisticupdates,gainnormalization,andadiversitybonus,thenblendedwithasmoothedbestpurestrategy.ThissolvercombinesthestabilityandconvergencepropertiesofOptimisticRegretMatching+(ORM+)withanexplicitpulltowardshighlyrewardingpurestrategies,smoothedbyatemperature-controlledsoftmax.ThishybridapproachaimstoleverageORM+â€™sabilitytofindmixedequilibriawhilealsoquicklyidentifyingandexploringstrongpure-strategymodesinthemeta-game,therebypotentiallyacceleratingthediscoveryoflow-exploitablepoliciesinPSRO.Theblendingfactorcontrolsthetrade-offbetweenthesetwodynamics.Args:meta_games:Alistofn-dimensionalnumpyarrays,oneperplayer.iterations:Numberofinternalsolveriterations.blending_factor:Weight(0to1)forblendingORM+outputwiththesmoothedbestpurestrategy.Afactorof0meanspureORM+;1meanspuresmoothedbestpurestrategy.temperature:Temperatureforsoftmaxsmoothingwhencalculatingthesmoothedbestpurestrategy.Lowervaluesmakethesmoothingsharper.momentum_beta:Momentumparameterforoptimisticupdatestopayoffgains.gain_normalization:IfTrue,normalizespayoffgainstomakelearningratemorerobustacrossgames.diversity_bonus_coeff:Coefficientfordiversitybonus,encouragingexplorationofless-chosenpolicies.return_average_strategy:IfTrue,returnstime-averagedstrategies.IfFalse,returnslast-iteratestrategies.Returns:Alistofmixed-strategies,oneforeachplayer,asnumpyarrays."""num_players=len(meta_games)num_strats=[m.shape[i]fori,minenumerate(meta_games)]ifany(n_s==0forn_sinnum_strats):return[np.array([]).tolist()for_inrange(num_players)]strategies=[np.ones(s,dtype=float)/sforsinnum_strats]cum_regrets=[np.zeros(s,dtype=float)forsinnum_strats]avg_strategies=[np.zeros(s,dtype=float)forsinnum_strats]prev_centered_payoff_gains=[np.zeros(s,dtype=float)forsinnum_strats]fortinrange(iterations):current_centered_payoff_gains=[np.zeros(s,dtype=float)forsinnum_strats]orm_strategies_this_iter=[np.zeros(s,dtype=float)forsinnum_strats]forpinrange(num_players):payoff_vec=meta_games[p]forother_pinreversed(range(num_players)):ifother_p!=p:payoff_vec=np.tensordot(payoff_vec,strategies[other_p],axes=([other_p],[0]))centered_payoff_gains=payoff_vec-np.mean(payoff_vec)current_centered_payoff_gains[p]=centered_payoff_gainsoptimistic_payoff_gains=(1+momentum_beta)*centered_payoff_gains-\momentum_beta*prev_centered_payoff_gains[p]diversity_bonus=diversity_bonus_coeff*(1.0-strategies[p])gains_for_regret_update=optimistic_payoff_gains+diversity_bonusifgain_normalization:max_abs_gain=np.max(np.abs(gains_for_regret_update))ifmax_abs_gain>1e-8:gains_for_regret_update/=max_abs_gaincum_regrets[p]+=gains_for_regret_updatecum_regrets[p]=np.maximum(0,cum_regrets[p])sum_pos_regret=cum_regrets[p].sum()ifsum_pos_regret>1e-12:orm_strategies_this_iter[p]=cum_regrets[p]/sum_pos_regretelse:orm_strategies_this_iter[p]=np.ones(num_strats[p])/num_strats[p]smoothed_best_pure=_smoothed_best_pure_strategy(payoff_vec,temperature)strategies[p]=(1-blending_factor)*orm_strategies_this_iter[p]+\blending_factor*smoothed_best_pureprev_centered_payoff_gains[p]=current_centered_payoff_gains[p]ifreturn_average_strategy:#Accumulateblendedstrategyonlyifaverageisrequestedavg_strategies[p]+=strategies[p]ifreturn_average_strategy:final_strategies=[]forpinrange(num_players):sum_avg_strat=np.sum(avg_strategies[p])ifsum_avg_strat>0:final_strategies.append(avg_strategies[p]/sum_avg_strat)else:final_strategies.append(np.ones(num_strats[p])/num_strats[p])returnfinal_strategieselse:#Ifnotreturningaverage,returnthelast-iteratestrategiesreturnstrategiesclassTrainMetaStrategySolver:"""Ahybridmeta-solverfortrainingthatblendsORM+withsmoothedbestpurestrategies.Thissolveraimstoaccelerateconvergencetolow-exploitablestrategiesbydynamicallybalancingregret-minimizationwithapulltowardshigh-performing(butsmoothed)purestrategies.Optimisticupdates,gainnormalization,andadiversitybonusareincorporatedforimprovedlearningdynamics.Theblendingfactor,temperature,anddiversitybonusareannealedovertheouterPSROiterations."""def__init__(self,base_solver_iterations=1000,#Basenumberofinternaliterationsiterations_per_policy_scale=20,#Howmuchiterationsscaleperaddedpolicymax_solver_iterations=5000,#Maxinternalsolveriterationsinitial_blending_factor=0.3,final_blending_factor=0.05,initial_temperature=0.5,final_temperature=0.01,momentum_beta=0.5,gain_normalization=True,initial_diversity_bonus_coeff=0.05,final_diversity_bonus_coeff=0.001,max_psro_iterations_for_annealing=75):"""InitializeshybridORMsolverparametersfortraining.Args:base_solver_iterations:Basenumberofinternalsolveriterationsfor_hybrid_orm_solver.iterations_per_policy_scale:Amounttoincreaseinternalsolveriterationsperaddedpolicy.max_solver_iterations:Maximuminternalsolveriterations.initial_blending_factor:Initialweightforthesmoothedbestpurestrategycomponent.final_blending_factor:Finalweightforthesmoothedbestpurestrategycomponent.initial_temperature:Initialtemperatureforsoftmaxsmoothing.final_temperature:Finaltemperatureforsoftmaxsmoothing.momentum_beta:Momentumforoptimisticupdates.gain_normalization:Normalizesgainsforscale-invariance.initial_diversity_bonus_coeff:Maxinitialdiversitybonuscoefficient.final_diversity_bonus_coeff:MininitialdiversitybonuscoefficientacrossPSROiterations.max_psro_iterations_for_annealing:PSROiterationsoverwhichouterannealingoccurs."""self._base_solver_iterations=base_solver_iterationsself._iterations_per_policy_scale=iterations_per_policy_scaleself._max_solver_iterations=max_solver_iterationsself._initial_blending_factor=initial_blending_factorself._final_blending_factor=final_blending_factorself._initial_temperature=initial_temperatureself._final_temperature=final_temperatureself._momentum_beta=momentum_betaself._gain_normalization=gain_normalizationself._initial_diversity_bonus_coeff=initial_diversity_bonus_coeffself._final_diversity_bonus_coeff=final_diversity_bonus_coeffself._max_psro_iterations_for_annealing=max_psro_iterations_for_annealingself._current_psro_iteration=0defget_meta_strategy(self,game,policy_sets,meta_games):"""Returnsblendedmetastrategiesfortraining.Args:game:Thepyspielgameobject.policy_sets:Alistoflistsofpolicies,onelistperplayer.policy_sets[p][i]isplayerpâ€™si-thpolicy.len(policy_sets[p])==meta_games[0].shape[p].meta_games:Alistofn-dimensionalnumpyarrays,oneperplayer.Eacharrayhasshape(num_strats_p0,num_strats_p1,...,num_strats_pn-1)andmeta_games[p][i0,i1,...,in-1]isthepayoffofplayerpwhenplayerkchoosesstrategyik.Returns:Alistofblendedmixed-strategies."""delgame,policy_sets#Unusedself._current_psro_iteration+=1current_psro_iter=self._current_psro_iteration#Adaptivesolveriterations:scalewithcurrentpopulationsizenum_current_policies_p0=len(meta_games[0])#Assumingsymmetricpopulationssolver_iterations=int(self._base_solver_iterations+self._iterations_per_policy_scale*(num_current_policies_p0-1))solver_iterations=np.clip(solver_iterations,self._base_solver_iterations,self._max_solver_iterations)annealing_progress=min(1.0,current_psro_iter/self._max_psro_iterations_for_annealing)blending_factor=(self._initial_blending_factor*(1.0-annealing_progress)+self._final_blending_factor*annealing_progress)temperature=(self._initial_temperature*(1.0-annealing_progress)+self._final_temperature*annealing_progress)diversity_bonus_coeff=(self._initial_diversity_bonus_coeff*(1.0-annealing_progress)+self._final_diversity_bonus_coeff*annealing_progress)blending_factor=np.clip(blending_factor,self._final_blending_factor,self._initial_blending_factor)temperature=np.clip(temperature,self._final_temperature,self._initial_temperature)diversity_bonus_coeff=np.clip(diversity_bonus_coeff,self._final_diversity_bonus_coeff,self._initial_diversity_bonus_coeff)strategies=_hybrid_orm_solver(meta_games,iterations=solver_iterations,#Useadaptiveiterationsblending_factor=blending_factor,temperature=temperature,momentum_beta=self._momentum_beta,gain_normalization=self._gain_normalization,diversity_bonus_coeff=diversity_bonus_coeff,return_average_strategy=True#Trainingalwaysusesaveragedstrategiesforstability)return[s.tolist()forsinstrategies]classEvalMetaStrategySolver:"""ReturnsmetastrategiesforevaluationinPSRO.Thissolverusesahybridapproach,blendingOptimisticRegretMatching+withasmoothedbestpurestrategy,tailoredforrobustandaccurateexploitabilitymeasurement.Theparametersaresettoemphasizeexploitationforevaluationpurposes,includingoptimisticupdatesandgainnormalizationforstability,whilekeepingdiversitybonusminimal.Crucially,itreturnsthe*last-iterate*strategyforareactiveestimateofexploitability."""def__init__(self,base_solver_iterations=8000,#Basenumberofinternaliterationsiterations_per_policy_scale=50,#Howmuchiterationsscaleperaddedpolicymax_solver_iterations=15000,#Maxinternalsolveriterationsblending_factor=0.01,temperature=0.001,momentum_beta=0.2,gain_normalization=True,diversity_bonus_coeff=0.0):"""InitializeshybridORMsolverparametersforevaluationmeta-strategies.Args:base_solver_iterations:Basenumberofinternalsolveriterationsfor_hybrid_orm_solver.iterations_per_policy_scale:Amounttoincreaseinternalsolveriterationsperaddedpolicy.max_solver_iterations:Maximuminternalsolveriterations.blending_factor:Weight(0to1)forthesmoothedbestpurestrategycomponent.temperature:Temperatureforsoftmaxsmoothing.momentum_beta:Momentumforoptimisticupdates.gain_normalization:Normalizesgainsforscale-invariance.diversity_bonus_coeff:Diversitybonus,keptverylowforevaluation."""self._base_solver_iterations=base_solver_iterationsself._iterations_per_policy_scale=iterations_per_policy_scaleself._max_solver_iterations=max_solver_iterationsself._blending_factor=blending_factorself._temperature=temperatureself._momentum_beta=momentum_betaself._gain_normalization=gain_normalizationself._diversity_bonus_coeff=diversity_bonus_coeffdefget_meta_strategy(self,game,policy_sets,meta_games):"""Returnsblendedmetastrategiesforevaluationinpolicy-spaceresponseoracles.Args:game:Thepyspielgameobject.policy_sets:Alistoflistsofpolicies,onelistperplayer.policy_sets[p][i]isplayerpâ€™si-thpolicy.len(policy_sets[p])==meta_games[0].shape[p].meta_games:Alistofn-dimensionalnumpyarrays,oneperplayer.Eacharrayhasshape(num_strats_p0,num_strats_p1,...,num_strats_pn-1)andmeta_games[p][i0,i1,...,in-1]isthepayoffofplayerpwhenplayerkchoosesstrategyik.Returns:Alistofmixed-strategies,oneforeachplayer.Eachmixedstrategyisalistofnon-negativeweights(notnecessarilynormalized).ItisusedforevaluationofthecurrentPSROpolicies.E.g.,computingexploitability."""delgame,policy_sets#Unusednum_current_policies_p0=len(meta_games[0])#Assumingsymmetricpopulationssolver_iterations=int(self._base_solver_iterations+self._iterations_per_policy_scale*(num_current_policies_p0-1))solver_iterations=np.clip(solver_iterations,self._base_solver_iterations,self._max_solver_iterations)strategies=_hybrid_orm_solver(meta_games,iterations=solver_iterations,#Useadaptiveiterationsblending_factor=self._blending_factor,temperature=self._temperature,momentum_beta=self._momentum_beta,gain_normalization=self._gain_normalization,diversity_bonus_coeff=self._diversity_bonus_coeff,return_average_strategy=False#Evalexplicitlyrequestslast-iteratestrategy)return[s.tolist()forsinstrategies]
### 7.2 Asymmetric Optimistic Discounted CFR

In an early trial, we discover another variant of CFR by training on 2-player Kuhn Poker, 2-player Leduc Poker, 4-Card Goofspiel and 4-side Liar Dice. It also shows good empirical performance and employs relatively more conventional mechanisms than VAD-CFR. We show its empirical performance together with other CFR variants in Figure 3. The source code is in Listing 7. Unlike manual heuristics which often rely on symmetric or fixed update rules, AOD-CFR utilizes a set of dynamic, asymmetric mechanisms discovered via evolutionary search.

- â€¢
Adaptive Regret Discounting: The algorithm employs a linear schedule for discounting cumulative regrets. The positive regret discount factor, Î±\alpha, transitions from 1.0â†’2.51.0\to 2.5 over 500 iterations, while the negative regret discount factor, Î²\beta, transitions from 0.5â†’0.00.5\to 0.0. This allows the algorithm to aggressively prune suboptimal actions early (via Î²\beta) while increasingly emphasizing recent information for optimal actions (via Î±\alpha).

- â€¢
Asymmetric Instantaneous Scaling: A distinct feature of AOD-CFR is its sign-dependent scaling of instantaneous regret. As revealed in the source code, the update rule scales instantaneous regret by 1.11.1 when both cumulative and instantaneous regrets are positive, but attenuates it by 0.90.9 when cumulative regret is positive and instantaneous regret is negative. This asymmetry likely functions as a momentum-preservation mechanism, reinforcing established beneficial actions while damping noise.

- â€¢
Trend-Based Policy Optimism: In the policy derivation step, AOD-CFR incorporates a â€œtrend-based optimismâ€ term. It tracks an Exponential Moving Average (EMA) of cumulative regrets (decay â‰ˆ0.1\approx 0.1) and adds a scaled deviation term to the accumulated regret before applying Regret Matching. Specifically, the code implements a scaling factor optimism_trend_scale applied to the difference between current cumulative regret and its EMA.

- â€¢
Polynomial Policy Averaging: The average strategy is computed using an adaptive polynomial weighting scheme where the exponent Î³\gamma scales linearly from 1.0â†’5.01.0\to 5.0.

Listing 7: AOD-CFRâ¬‡importmathimportcollectionsimportattrclassRegretAccumulator:"""Updatescumulativeregretwithadaptivepositiveandnegativediscounting,andasymmetricinstantaneousregretscaling."""def__init__(self,alpha_start:float=1.0,alpha_max:float=2.5,schedule_T_alpha:float=500.0,beta_start:float=0.5,beta_max:float=0.0,schedule_T_beta:float=500.0,pos_cum_pos_inst_scale:float=1.1,#R_cum>0,r_inst>0pos_cum_neg_inst_scale:float=0.9,#R_cum>0,r_inst<0neg_cum_pos_inst_scale:float=0.7,#R_cum<0,r_inst>0neg_cum_neg_inst_scale:float=1.2):#R_cum<0,r_inst<0"""Initializestheregretaccumulator.Args:alpha_start:Initialexponentforpositivecumulativeregretdiscounting.alpha_max:Finalexponentforpositivecumulativeregretdiscounting.schedule_T_alpha:Iterationsforalphatransition.beta_start:Initialexponentfornegativecumulativeregretdiscounting.beta_max:Finalexponentfornegativecumulativeregretdiscounting.schedule_T_beta:Iterationsforbetatransition.pos_cum_pos_inst_scale:Scalingfactorforinstantaneousregretwhencumulativeregretispositiveandinstantaneousispositive.pos_cum_neg_inst_scale:Scalingfactorforinstantaneousregretwhencumulativeregretispositiveandinstantaneousisnegative.neg_cum_pos_inst_scale:Scalingfactorforinstantaneousregretwhencumulativeregretisnegativeandinstantaneousispositive.neg_cum_neg_inst_scale:Scalingfactorforinstantaneousregretwhencumulativeregretisnegativeandinstantaneousisnegative."""ifnot(alpha_start>=0andalpha_max>=0andschedule_T_alpha>=0):raiseValueError("Alphaparametersandschedulemustbenon-negative.")ifnot(beta_start>=0andbeta_max>=0andschedule_T_beta>=0):raiseValueError("Betaparametersandschedulemustbenon-negative.")ifnot(pos_cum_pos_inst_scale>=0andpos_cum_neg_inst_scale>=0andneg_cum_pos_inst_scale>=0andneg_cum_neg_inst_scale>=0):raiseValueError("Instantaneousregretscalingfactorsmustbenon-negative.")self._alpha_start=alpha_startself._alpha_max=alpha_maxself._schedule_T_alpha=max(1.0,schedule_T_alpha)self._beta_start=beta_startself._beta_max=beta_maxself._schedule_T_beta=max(1.0,schedule_T_beta)self._pos_cum_pos_inst_scale=pos_cum_pos_inst_scaleself._pos_cum_neg_inst_scale=pos_cum_neg_inst_scaleself._neg_cum_pos_inst_scale=neg_cum_pos_inst_scaleself._neg_cum_neg_inst_scale=neg_cum_neg_inst_scaleself._epsilon=1e-9#Smallvalueforsigncomparisonsdefupdate_accumulate_regret(self,info_state_node,iteration_number,cfr_regrets):"""Updatescumulativeregretswithadaptivediscountingandasymmetricinstantaneousregretscaling.Args:info_state_node:Datastructurefortheinformationset.iteration_number:ThecurrentCFRiteration(0-based).cfr_regrets:Instantaneouscounterfactualregretsforthecurrentiteration.Returns:Updatedcumulativeregretdictionaryforeachaction."""t=float(iteration_number)+1.0#1-indexedforweightingcalculationsiter_float=float(iteration_number)#0-basedforscheduleprogress#Calculatecurrent_alphaforpositivecumulativeregretdiscountingt_norm_alpha=min(1.0,iter_float/self._schedule_T_alpha)current_alpha=self._alpha_start+(self._alpha_max-self._alpha_start)*t_norm_alpha#Calculatecurrent_betafornegativecumulativeregretdiscountingt_norm_beta=min(1.0,iter_float/self._schedule_T_beta)current_beta=self._beta_start+(self._beta_max-self._beta_start)*t_norm_betaupdated_cumulative_regret={}legal_actions=info_state_node.legal_actionsforactioninlegal_actions:prev_cumulative_regret=info_state_node.cumulative_regret.get(action,0.0)current_instantaneous_regret=cfr_regrets.get(action,0.0)#Determineinstantaneousregretscalingfactorbasedonsignsinstantaneous_scaling_factor=1.0#Defaultifprev_cumulative_regret>self._epsilon:#Cumulativeregretispositiveifcurrent_instantaneous_regret>self._epsilon:instantaneous_scaling_factor=self._pos_cum_pos_inst_scaleelifcurrent_instantaneous_regret<-self._epsilon:instantaneous_scaling_factor=self._pos_cum_neg_inst_scaleelifprev_cumulative_regret<-self._epsilon:#Cumulativeregretisnegativeifcurrent_instantaneous_regret>self._epsilon:instantaneous_scaling_factor=self._neg_cum_pos_inst_scaleelifcurrent_instantaneous_regret<-self._epsilon:instantaneous_scaling_factor=self._neg_cum_neg_inst_scale#Ifoneorbotharenearzero,instantaneous_scaling_factorremains1.0scaled_instantaneous_regret=current_instantaneous_regret*instantaneous_scaling_factor#Calculatediscountfactorforthepreviouscumulativeregretbasedonitssigndiscount_factor=0.0ifprev_cumulative_regret>0:try:t_pow_alpha=math.pow(t,current_alpha)#Ift^alphaoverflows,thefactorapproaches1.0discount_factor=t_pow_alpha/(t_pow_alpha+1.0)ift_pow_alpha!=float(â€™infâ€™)else1.0exceptValueError:#Fallbackforpotentialissueswithverylargetandalphadiscount_factor=1.0ift>1.1else0.5#t=1is0.5(1/2),largetis1.0(inf/inf)else:#prev_cumulative_regretisnon-positivetry:t_pow_beta=math.pow(t,current_beta)#Ift^betaoverflows,thefactorapproaches1.0discount_factor=t_pow_beta/(t_pow_beta+1.0)ift_pow_beta!=float(â€™infâ€™)else1.0exceptValueError:#Fallbackforpotentialissueswithverylargetandbetadiscount_factor=1.0ift>1.1else0.5#t=1is0.5(1/2),largetis1.0(inf/inf)discounted_prev_regret=prev_cumulative_regret*discount_factor#Addthescaledcurrentinstantaneousregret.new_cumulative_regret=discounted_prev_regret+scaled_instantaneous_regretupdated_cumulative_regret[action]=new_cumulative_regretreturnupdated_cumulative_regretimportmathimportcollectionsimportattr#EnsureattrisimportedifnotalreadygloballyclassPolicyFromRegretAccumulator:"""CalculatesthecurrentpolicyusingcumulativeregretsplusanoptimistictermbasedonthedeviationofcurrentcumulativeregretfromitsEMA."""def__init__(self,use_squared_weights:bool=True,cumulative_regret_ema_alpha:float=0.1,#EMAfactorforcumulativeregrets(lowermeansmoresmoothing)optimism_trend_scale:float=0.5):#ScalingfactorfortheR^t-EMA(R^{t-1})term"""Initializesthepolicycalculatorwithtrend-basedoptimism.Args:use_squared_weights:IfTrue(default),usemax(0,R+scaled_optimism)^2forpolicyweights(CFR+style).IfFalse,usemax(0,R+scaled_optimism)forpolicyweights(RM+style).cumulative_regret_ema_alpha:DecayfactorfortheEMAofcumulativeregrets(0<alpha<=1).Higheralphagivesmoreweighttorecentcumulativeregrets.optimism_trend_scale:ScalingfactorappliedtothedifferencebetweencurrentcumulativeregretanditsEMA(R^t-EMA(R^{t-1}))beforeaddingittoR^t."""ifnot(0.0<cumulative_regret_ema_alpha<=1.0):raiseValueError("cumulative_regret_ema_alphamustbebetween0(exclusive)and1(inclusive).")ifoptimism_trend_scale<0:raiseValueError("optimism_trend_scalemustbenon-negative.")self._use_squared_weights=use_squared_weightsself._cumulative_regret_ema_alpha=cumulative_regret_ema_alphaself._optimism_trend_scale=optimism_trend_scale#StoreEMAof*cumulative*regrets:{infoset_index:{action:ema_value}}self._ema_cumulative_regrets=collections.defaultdict(lambda:collections.defaultdict(float))#Epsilonforsafefloatingpointcomparisonsforpolicynormalization.self._epsilon_norm=1e-12defget_updated_current_policy(self,info_state_node,iteration_number,cfr_regrets,previous_policy):"""Calculatesthecurrentpolicyusingcumulativeregretsplustrend-basedoptimism.Args:info_state_node:adatastructurecorrespondingtoaninformationsetwithcumulative_regretandcumulative_policystored.cumulative_regret(R^t)hasbeenupdatedbyRegretAccumulatorforthecurrentiteration.iteration_number:thecurrentCFRiteration(0-based)(unusedbutkeptforAPI).cfr_regrets:theinstantaneouscounterfactualregrets(r^t)forthecurrentiteration(unused).previous_policy:thepreviouspolicyatthecurrentinformationset(unused).Returns:updatedcurrentpolicydictionaryatthecurrentinformationset."""cumulative_regrets_map=info_state_node.cumulative_regret#R^tactions=info_state_node.legal_actionsinfoset_index=info_state_node.index_in_tabular_policy#UniquekeyforEMAstorageaction_to_policy_weight={}sum_policy_weights=0.0action_to_new_ema={}#StorenewEMAvaluesbeforeoverwritingstateifnotactions:#Handlecaseswithnolegalactionsreturn{}num_actions=len(actions)#Usedforuniformpolicyfallback#GettheEMAmapforthisspecificinfosetinfoset_ema_map=self._ema_cumulative_regrets[infoset_index]foractioninactions:#GetthecurrentcumulativeregretR^t(a)current_cum_regret=cumulative_regrets_map.get(action,0.0)#GetthepreviousEMAofcumulativeregretEMA(R^{t-1})(a)prev_ema_cum_regret=infoset_ema_map.get(action,0.0)#Defaultsto0.0onfirstencounter#Calculatethedeviation=R^t(a)-EMA(R^{t-1})(a)regret_trend=current_cum_regret-prev_ema_cum_regret#Calculatethescaledoptimistictermscaled_optimism=self._optimism_trend_scale*regret_trend#CalculatethevalueusedforRegretMatching:R^t(a)+scaled_optimismcombined_value=current_cum_regret+scaled_optimism#ApplyRegretMatchinglogic:takethepositivepart.positive_combined_value=max(0.0,combined_value)#Uselinearorsquaredweightingbasedontheflagifself._use_squared_weights:policy_weight=positive_combined_value**2#CFR+styleelse:policy_weight=positive_combined_value#RM+styleaction_to_policy_weight[action]=policy_weightsum_policy_weights+=policy_weight#UpdatetheEMAofcumulativeregretforthenextiteration(t)#EMA^t=alpha*R^t+(1-alpha)*EMA^{t-1}new_ema_cum_regret=self._cumulative_regret_ema_alpha*current_cum_regret+\(1.0-self._cumulative_regret_ema_alpha)*prev_ema_cum_regretaction_to_new_ema[action]=new_ema_cum_regret#UpdatethestoredEMAvalues*after*usingthepreviousvaluesforallactionsforactioninactions:infoset_ema_map[action]=action_to_new_ema[action]#Normalizetogetthepolicyinfo_state_policy={}ifsum_policy_weights>self._epsilon_norm:#Usenormalizationepsilon#Normalizepositiveweights(ortheirsquares)togetpolicyprobabilitiesforactioninactions:info_state_policy[action]=action_to_policy_weight[action]/sum_policy_weightselse:#Defaulttouniformpolicyifsumofweightsiszeroornumericallyclosetozerouniform_prob=1.0/num_actionsifnum_actions>0else0.0foractioninactions:info_state_policy[action]=uniform_probreturninfo_state_policyclassPolicyAccumulator:"""Updatescumulativepolicyusingadaptivepolynomialweighting(likeDCFR/PDCFR+)."""def__init__(self,gamma_start:float=1.0,gamma_max:float=5.0,gamma_schedule_T:float=500.0):"""Initializesthepolicyaccumulatorwithadaptivegamma.Args:gamma_start:Initialexponentforweightingthecurrentpolicy(t^gamma).gamma_max:Maxgammavalueforlaterexploitation.gamma_schedule_T:Numberofiterationsoverwhichgammatransitions."""ifgamma_start<0orgamma_max<0orgamma_schedule_T<0:raiseValueError("PolicyAccumulatorgammaparametersmustbenon-negative.")self._gamma_start=gamma_startself._gamma_max=gamma_maxself._gamma_schedule_T=max(1.0,gamma_schedule_T)#Avoiddivisionbyzerodefupdate_accumulate_policy(self,info_state_node,iteration_number,info_state_policy,cfr_regrets,#Notusedreach_prob,counterfactual_reach_prob,#Notused):"""Updatesthecumulativepolicyusingadaptivepolynomialweighting.Args:info_state_node:Datastructurefortheinformationset.iteration_number:ThecurrentCFRiteration(0-based).info_state_policy:Thecurrentpolicyprofileforthisiteration.reach_prob:Playerâ€™sreachprobabilityforthisiteration.Returns:Updatedcumulativepolicydictionaryforeachaction."""#Use1-basediterationnumbertforcalculationst=float(iteration_number)+1.0#Use0-basediterationforscheduleprogresscalculationiter_float=float(iteration_number)#Calculatecurrent_gammabasedonlinearschedulet_norm_gamma=min(1.0,iter_float/self._gamma_schedule_T)current_gamma=self._gamma_start+(self._gamma_max-self._gamma_start)*t_norm_gamma#Calculatet^current_gamma,whichisusedforbothdiscountingandweighting.t_pow_gamma=1.0ift>0:#tis1-indexed(float(iteration_number)+1.0),sot=0isnotpossiblehere.try:t_pow_gamma=math.pow(t,current_gamma)exceptValueError:#Fallbackforextremelylargeexponentstoavoidoverflow,treatasverylarge.t_pow_gamma=float(â€™infâ€™)#Calculatethediscountfactorforthepreviouscumulativepolicysum.#ThismirrorsthediscountinglogicusedforpositivecumulativeregretsinRegretAccumulator.#Ift_pow_gammaisâ€™infâ€™,discountfactorapproaches1.0.policy_discount_factor=t_pow_gamma/(t_pow_gamma+1.0)ift_pow_gamma!=float(â€™infâ€™)else1.0#Calculatetheweightforthecurrentpolicyâ€™scontribution(AdaptiveCFRstyle).#Thisweightsthecurrentpolicybyt^gammaandtheplayerâ€™sreachprobability.current_policy_contribution_weight=t_pow_gamma*reach_probupdated_cumulative_policy={}#Iterateoveralllegalactionsandactionspresentinthecumulativepolicy#toensureproperdiscountingandupdatingofallrelevantentries.all_actions=set(info_state_node.legal_actions)|set(info_state_node.cumulative_policy.keys())foractioninall_actions:previous_cumulative_policy=info_state_node.cumulative_policy.get(action,0.0)current_policy_prob=info_state_policy.get(action,0.0)#Discountthepreviouslyaccumulatedpolicysum.discounted_previous_cumulative_policy=previous_cumulative_policy*policy_discount_factor#Addthecurrentpolicyâ€™sweightedcontribution.current_contribution=current_policy_contribution_weight*current_policy_probupdated_cumulative_policy[action]=(discounted_previous_cumulative_policy+current_contribution)#Ensureall*legal*actionsarepresentinthefinaldict,evenifvalueis0.0foractionininfo_state_node.legal_actions:ifactionnotinupdated_cumulative_policy:#Thishappensifanactionwaslegalbutneveraccumulatedpolicybefore#andisnâ€™tincurrentpolicy(prob=0).Itsvalueshouldbe0#afterdiscountingtheprevious0andadding0contribution.updated_cumulative_policy[action]=0.0returnupdated_cumulative_policy
### 7.3 Results on 11 Games
Figure 3: CFR variants performances on All Games.Figure 4: PSRO variants performances on All Games.
### 7.4 Prompts
Listing 8: Prompt for Evolving CFRâ¬‡Actasanexpertingametheory,multiagentlearning,onlinelearningandoptimization.Yourtaskistoiterativelyimproveanewvariantofcounterfactualregretminimization.Theprimarygoalistospeedupconvergencetolow-exploitablestrategies.AlwaysadheretobestpracticesinPythoncoding.Akeydatastructurethatisusedisinfostate_node:â€˜â€˜â€˜python@attr.sclass_InfoStateNode(object):"""Anobjectwrappingvaluesassociatedtoaninformationstate."""#Thelistofthelegalactions.legal_actions=attr.ib()index_in_tabular_policy=attr.ib()#Mapfrominformationstatesstringrepresentationsandactionstothe#counterfactualregrets,accumulatedoverthepolicyiterationscumulative_regret=attr.ib(factory=lambda:collections.defaultdict(float))#Sameasaboveforthecumulativeofthepolicyprobabilitiescomputed#duringthepolicyiterationscumulative_policy=attr.ib(factory=lambda:collections.defaultdict(float))â€˜â€˜â€˜YouareallowedtomodifythreekeycomponentsofCFR:(1)howaretheregretvaluesaccumulatedateachinfoset(RegretAccumulator)(2)howtoobtainacurrentpolicyatthecurrentiterationfromthecurrentcumulative_regret(PolicyFromRegretAccumulator)and(3)howtoaccumulatepoliciesacrossiterationstocomputeanaveragepolicy(PolicyAccumulator).#PriorprogramsPreviouslywefoundthatthefollowingprogramsperformedwellonthetaskathand:{previous_programs}#CurrentprogramHereisthecurrentprogramwearetryingtoimprove(youwillneedtoproposeamodificationtoitbelow):{code}#*SEARCH/REPLACEblock*Rules:Every*SEARCH/REPLACEblock*mustusethisformat:1.Theopeningfence:â€˜â€˜â€˜python2.Thestartofsearchblock:<<<<<<<SEARCH3.Acontiguouschunkofupto4linestosearchforintheexistingsourcecode4.Thedividingline:=======5.Thelinestoreplaceintothesourcecode6.Theendofthereplaceblock:>>>>>>>REPLACE7.Theclosingfence:â€˜â€˜â€˜Every*SEARCH*sectionmust*EXACTLYMATCH*theexistingfilecontent,characterforcharacter,includingallcomments,docstrings,etc.*SEARCH/REPLACE*blockswillreplace*all*matchingoccurrences.IncludeenoughlinestomaketheSEARCHblocksuniquelymatchthelinestochange.Keep*SEARCH/REPLACE*blocksconcise.Breaklarge*SEARCH/REPLACE*blocksintoaseriesofsmallerblocksthateachchangeasmallportionofthefile.Includejustthechanginglines,andafewsurroundinglinesifneededforuniqueness.Donotincludelongrunsofunchanginglinesin*SEARCH/REPLACE*blocks.Tomovecodewithinafile,use2*SEARCH/REPLACE*blocks:1todeleteitfromitscurrentlocation,1toinsertitinthenewlocation.Makesurethatthechangesyouproposeareconsistentwitheachother.Forexample,ifyourefertoanewconfigvariablesomewhere,youshouldalsoproposeachangetoaddthatvariable.Example:â€˜â€˜â€˜python<<<<<<<SEARCHreturntotal_loss=======#Addsparsity-promotingregularizationtotheloss.total_loss+=self.hypers.l1_reg_weight*l1_regreturntotal_loss{replace}â€˜â€˜â€˜andâ€˜â€˜â€˜python<<<<<<<SEARCHreturnhyper.zipit([=======returnhyper.zipit([hyper.uniform(â€™l1_reg_weightâ€™,hyper.interval(0.0,0.01)),{replace}â€˜â€˜â€˜{lazy_prompt}ONLYEVERRETURNCODEINA*SEARCH/REPLACEBLOCK*!#Task{task_instruction}{focus_sentence}{trigger_chain_of_thought}Describeeachchangewitha*SEARCH/REPLACEblock*.Listing 9: Prompt for Evolving PSROâ¬‡Actasanexpertingametheory,multiagentlearning,onlinelearningandoptimization.YourtaskistoiterativelyimproveavariantofPolicy-SpaceResponseOracles(PSRO).Theprimarygoalistospeedupconvergencetolow-exploitablestrategies.#PSROOverviewPSROiterativelybuildsapopulationofpoliciesforeachplayerandcomputesmeta-strategies(distributionsoverpolicies)toguidetrainingandevaluation.**Eachiteration:**1.**Empiricalgame**:Simulatepayoffsbetweenallpolicycombinationstoformagametensor.2.**Train-timemeta-strategy**:Computeadistributionovercurrentpoliciesforeachplayer.Thisdetermineswhatopponentsthebest-responseoracletrainsagainst.3.**Bestresponse**:Addanewpolicyforeachplayerthatbestrespondstoopponentsâ€™train-timemeta-strategies.4.**Eval-timemeta-strategy**:Computea(possiblydifferent)distributionforevaluation,e.g.,tomeasureexploitability.**Yourtask**:Improveboththe**train-time**and**eval-time**meta-strategysolvers.Theseservedifferentpurposes:train-timeguidespopulationgrowth,eval-timeassessessolutionquality.#AvailableUtilities**BestResponse**-â€˜BestResponsePolicy(game,player_id,policy)â€˜:Returnsabest-responsepolicyforâ€˜player_idâ€˜againstâ€˜policyâ€˜.-Useâ€˜.value(game.new_initial_state())â€˜togettheBRvalue.**PolicyAggregation**-â€˜PolicyAggregator(game).aggregate(pids,policy_sets,weights)â€˜:Createsamixedpolicyfromweightedpurepolicies.-â€˜pidsâ€˜:â€˜list(range(game.num_players()))â€˜-â€˜weightsâ€˜:â€˜[weights_p0,weights_p1,...]â€˜,eachmatchingâ€˜len(policy_sets[p])â€˜**PolicyEvaluation**-â€˜expected_game_score.policy_value(state,joint_policy)â€˜:Returnsexpectedpayoffs(list,oneperplayer).-â€˜joint_policyâ€˜:â€˜[policy_p0,policy_p1,...]â€˜AlwaysadheretobestpracticesinPythoncoding.#PriorprogramsPreviouslywefoundthatthefollowingprogramsperformedwellonthetaskathand:{previous_programs}#CurrentprogramHereisthecurrentprogramwearetryingtoimprove(youwillneedtoproposeamodificationtoitbelow):{code}#*SEARCH/REPLACEblock*Rules:Every*SEARCH/REPLACEblock*mustusethisformat:1.Theopeningfence:â€˜â€˜â€˜python2.Thestartofsearchblock:<<<<<<<SEARCH3.Acontiguouschunkofupto4linestosearchforintheexistingsourcecode4.Thedividingline:=======5.Thelinestoreplaceintothesourcecode6.Theendofthereplaceblock:>>>>>>>REPLACE7.Theclosingfence:â€˜â€˜â€˜Every*SEARCH*sectionmust*EXACTLYMATCH*theexistingfilecontent,characterforcharacter,includingallcomments,docstrings,etc.*SEARCH/REPLACE*blockswillreplace*all*matchingoccurrences.IncludeenoughlinestomaketheSEARCHblocksuniquelymatchthelinestochange.Keep*SEARCH/REPLACE*blocksconcise.Breaklarge*SEARCH/REPLACE*blocksintoaseriesofsmallerblocksthateachchangeasmallportionofthefile.Includejustthechanginglines,andafewsurroundinglinesifneededforuniqueness.Donotincludelongrunsofunchanginglinesin*SEARCH/REPLACE*blocks.Tomovecodewithinafile,use2*SEARCH/REPLACE*blocks:1todeleteitfromitscurrentlocation,1toinsertitinthenewlocation.Makesurethatthechangesyouproposeareconsistentwitheachother.Forexample,ifyourefertoanewconfigvariablesomewhere,youshouldalsoproposeachangetoaddthatvariable.Example:â€˜â€˜â€˜python<<<<<<<SEARCHreturntotal_loss=======#Addsparsity-promotingregularizationtotheloss.total_loss+=self.hypers.l1_reg_weight*l1_regreturntotal_loss{replace}â€˜â€˜â€˜andâ€˜â€˜â€˜python<<<<<<<SEARCHreturnhyper.zipit([=======returnhyper.zipit([hyper.uniform(â€™l1_reg_weightâ€™,hyper.interval(0.0,0.01)),{replace}â€˜â€˜â€˜{lazy_prompt}ONLYEVERRETURNCODEINA*SEARCH/REPLACEBLOCK*!#Task{task_instruction}{focus_sentence}{difficulty_hint}{exploration_nudge}{trigger_chain_of_thought}Describeeachchangewitha*SEARCH/REPLACEblock*.
